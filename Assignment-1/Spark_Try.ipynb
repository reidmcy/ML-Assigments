{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Spark Locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7ffb8c1e5750>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark session is now the entry point of spark program\n",
    "# for line 8, can use local[n] for run spark locally with n cores\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('Spark Try') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Spark Standalone Cluster\n",
    "\n",
    "The following piece of code will run Spark on its standalone cluster mode. Please don't redo this before you stop a cluster...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Master web UI: http://10.50.221.12:8080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# start spark master program on this machine and register worker nodes\n",
    "os.system('start-spark-slurm.sh&') # use & to put it into background\n",
    "\n",
    "# get ip address of this machine\n",
    "ip = os.popen('hostname -i').read().strip('\\n')\n",
    "print 'Spark Master web UI: http://{}:8080'.format(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a session and open web UI (you have it when you run spark locally, too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session web UI: http://10.50.221.12:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "    \n",
    "# get ip address of master node (current machine)\n",
    "ip = os.popen('hostname -i').read().strip('\\n')\n",
    "\n",
    "# change 'local' to be ip of master node\n",
    "spark = SparkSession.builder \\\n",
    "    .master('spark://' + ip + ':7077') \\\n",
    "    .appName('Spark Try') \\\n",
    "    .getOrCreate()\n",
    "print 'Spark Session web UI: http://{}:4040'.format(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Small Map/Reduce run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.98142396776e+13\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "npart = 200\n",
    "data = spark.sparkContext.parallelize(xrange(1000000000), npart)\n",
    "print data.map(lambda x: math.sqrt(float(x*2))).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop session & cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()                      # Spark Session web UI will stop\n",
    "os.system('stop-spark-slurm.sh')  # Spark Master web UI will stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First View of `RDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "number of partitions: 10\n",
      "<type 'list'> 10000\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "99990000\n",
      "99990000\n",
      "[u'Michael, 29', u'Andy, 30', u'Justin, 19']\n",
      "Michael__Andy__Justin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# spark session is now the entry point of spark program\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('Spark Try') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# create RDD from in memory collections\n",
    "npartition = 10\n",
    "data_to_dist = xrange(10000)\n",
    "data = spark.sparkContext.parallelize(data_to_dist, npartition)\n",
    "print type(data)\n",
    "print 'number of partitions: {}'.format(data.getNumPartitions())\n",
    "\n",
    "# get all data\n",
    "print type(data.collect()), len(data.collect())\n",
    "\n",
    "# get first 10 entries\n",
    "print data.take(10)\n",
    "\n",
    "# a small map reduce\n",
    "def f(x):\n",
    "    return x*2\n",
    "print data.map(f).reduce(lambda a, b: a + b)\n",
    "print sum([i*2 for i in data_to_dist])\n",
    "    \n",
    "# create RDD from file\n",
    "lines = spark.sparkContext.textFile('/software/spark-2.1-el7-x86_64/examples/src/main/resources/people.txt')\n",
    "print lines.collect()\n",
    "print lines.map(lambda l: l.split(',')[0]).reduce(lambda a, b: a + '__' + b)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataFrame` from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+--------------------+----+\n",
      "|           president|                text|year|\n",
      "+--------------------+--------------------+----+\n",
      "|        James Monroe| Fellow-Citizens ...|1821|\n",
      "|    William McKinley| To the Senate an...|1897|\n",
      "|Dwight D. Eisenhower|[Delivered in per...|1960|\n",
      "|     Calvin Coolidge|Since the close o...|1923|\n",
      "|       James Madison| Fellow-Citizens ...|1816|\n",
      "|    Grover Cleveland| To the Congress ...|1886|\n",
      "|   John Quincy Adams| Fellow Citizens ...|1827|\n",
      "|  Theodore Roosevelt| To the Senate an...|1905|\n",
      "|   Lyndon B. Johnson|Mr. Speaker, Mr. ...|1965|\n",
      "|       James K. Polk| Fellow-Citizens ...|1848|\n",
      "|      Woodrow Wilson|Gentlemen of the ...|1913|\n",
      "|Dwight D. Eisenhower|[Delivered in per...|1955|\n",
      "|         George Bush|Mr. President and...|1991|\n",
      "|     Franklin Pierce| Fellow-Citizens ...|1856|\n",
      "...|1944| D. Roose...|To the Congress:\n",
      "|   Lyndon B. Johnson|[Delivered in per...|1968|\n",
      "|      Andrew Johnson| Fellow-Citizens ...|1867|\n",
      "|      Woodrow Wilson|GENTLEMEN OF THE ...|1914|\n",
      "|      Harry S Truman|[As delivered in ...|1950|\n",
      "|      James Buchanan|Fellow-Citizens o...|1860|\n",
      "+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|           president|count|\n",
      "+--------------------+-----+\n",
      "|Franklin D. Roose...|   12|\n",
      "|      Andrew Johnson|    4|\n",
      "|   George Washington|    7|\n",
      "|     Calvin Coolidge|    6|\n",
      "|      Andrew Jackson|    8|\n",
      "|       James K. Polk|    4|\n",
      "|      Gerald R. Ford|    3|\n",
      "|    Grover Cleveland|    8|\n",
      "+--------------------+-----+\n",
      "only showing top 8 rows\n",
      "\n",
      "root\n",
      " |-- president: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n",
      "+------------+--------------------+----+\n",
      "|   president|                text|year|\n",
      "+------------+--------------------+----+\n",
      "|Barack Obama|Mr. Speaker, Mr. ...|2013|\n",
      "|Barack Obama|Mr. Speaker, Mr. ...|2011|\n",
      "|Barack Obama|Mr. Speaker, Mr. ...|2012|\n",
      "+------------+--------------------+----+\n",
      "\n",
      "James Monroe (1821):\n",
      "------------------------------\n",
      " Fellow-Citizens of the Senate and House of Representatives:\n",
      "\n",
      " \n",
      "\n",
      "The progress of our affairs since the last session has been such as may justly be claimed and expected under a Government deriving all its powers from an enlightened people, and under laws formed by their representatives, on great consideration, for the sole purpose of promoting the welfare and happiness of their constituents. In the execution of those laws and of the powers vested by the Constitution in the Executive, unremitt...\n",
      "\n",
      "\n",
      "William McKinley (1897):\n",
      "------------------------------\n",
      " To the Senate and House of Representatives:\n",
      "\n",
      " \n",
      "\n",
      "It gives me pleasure to extend greeting to the Fifty-fifth Congress, assembled in regular session at the seat of Government, with many of whose Senators and Representatives I have been associated in the legislative service. Their meeting occurs under felicitous conditions, justifying sincere congratulation and calling for our grateful acknowledgment to a beneficent Providence which has so signally blessed and prospered us as a nation. Peace an...\n",
      "\n",
      "\n",
      "Dwight D. Eisenhower (1960):\n",
      "------------------------------\n",
      "[Delivered in person before a joint session] \n",
      "\n",
      "Mr. President, Mr. Speaker, Members of the 86th Congress: \n",
      "\n",
      "Seven years ago I entered my present office with one long-held resolve overriding all others. I was then, and remain now, determined that the United States shall become an ever more potent resource for the cause of peace--realizing that peace cannot be for ourselves alone, but for peoples everywhere. This determination is shared by the entire Congress--indeed, by all Americans. \n",
      "\n",
      "My p...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "print type(df)\n",
    "df.show()\n",
    "\n",
    "# SQL-like operations\n",
    "df.groupBy('president').count().show(8)\n",
    "\n",
    "# the data frame is untyped...\n",
    "df.printSchema()\n",
    "df.filter(df['year'] > 2010).show()\n",
    "\n",
    "# print first 500 chars of speeches\n",
    "# df.rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "speech = df.rdd.map(lambda x: [(x['president'], int(x['year']), x['text'])]).reduce(lambda a, b: a+b)\n",
    "for j in xrange(3):\n",
    "    print \"%s (%d):\\n%s\\n%s...\\n\\n\" % (speech[j][0], speech[j][1], 30*\"-\", speech[j][2][0:500])\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DataFrame` from `RDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n",
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "spark  = SparkSession.builder.appName('spark_try').getOrCreate()\n",
    "\n",
    "# can also create dataset from RDD\n",
    "lines = spark.sparkContext.textFile('/software/spark-2.1-el7-x86_64/examples/src/main/resources/people.txt')\n",
    "ppl = lines.map(lambda l: l.split(\",\")).map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(ppl)\n",
    "schemaPeople.show()\n",
    "schemaPeople.createOrReplaceTempView('people')\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "teenNames = teenagers.rdd.map(lambda p: 'Name: ' + p['name']).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark is creating more high level abstraction. `DataFrame` encapsulates RDD and support SQL-like queries. `pyspark.ml`  uses `DataFrame`. But sometimes lower level abstractions are easier to process.... `pyspark.mllib` uses `RDD`. We could be flexible according to our datastructure and context.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Lab - print word count of speeches per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5765, 12019, 5633, 6657, 3337, 14927, 6887, 24925, 4406, 21124, 3531, 7233, 3968, 10398, 3693, 4812, 11930, 4507, 5133, 13913, 9950, 2054, 7225, 19329, 27243, 3825, 12097, 14527, 3197, 5502, 6902, 4825, 2284, 6438, 13346, 19434, 2189, 4604, 6053, 3405, 4467, 27731, 4468, 12884, 11367, 10710, 5341, 1950, 7277, 5248, 9173, 3105, 5368, 16030, 1393, 3256, 4707, 7475, 4431, 3197, 6860, 12270, 4944, 6384, 2840, 9692, 6749, 5534, 9514, 3650, 2200, 7378, 15691, 12266, 6062, 7090, 13131, 2687, 2659, 3212, 8736, 8257, 1816, 14992, 5591, 19501, 4948, 2086, 8261, 3472, 3466, 6874, 11524, 8624, 9185, 14853, 2090, 2422, 4465, 3221, 7227, 3454, 4919, 7989, 13815, 4708, 10751, 23667, 2841, 1355, 4559, 13321, 7816, 23466, 4821, 2260, 11410, 10094, 2894, 25130, 8497, 4377, 1654, 4140, 9274, 5336, 5432, 7812, 7804, 9820, 4658, 7659, 6141, 5548, 12106, 8020, 8337, 2074, 7026, 3770, 8206, 5287, 2248, 5552, 10448, 13585, 3208, 13407, 4104, 5676, 9856, 27913, 18119, 7120, 4144, 10846, 2687, 15385, 7975, 9603, 3414, 4725, 13548, 6938, 4212, 17295, 11424, 5981, 1970, 5157, 4096, 3761, 4362, 7560, 3952, 10782, 6040, 5082, 5171, 8958, 6951, 4455, 16280, 5094, 16174, 2081, 18910, 6189, 2907, 5167, 7629, 22658, 9749, 3415, 3811, 8928, 6997, 10595, 8679, 4336, 5702, 10255, 16236, 6726, 1500, 2191, 34661, 20024, 3119, 5045, 6328, 2368, 6297, 13374, 10325, 7645, 3882, 3981, 4947, 11509, 5178, 7129, 9052, 8369, 8926, 13112]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark  = SparkSession.builder.master('local').appName('SOU').getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/sou/speeches.json')\n",
    "\n",
    "print df.rdd.map(lambda x: len(x['text'].split(' '))).collect()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
