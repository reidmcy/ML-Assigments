{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"mnist\").getOrCreate()\n",
    "df = spark.read.json('/project/cmsc25025/mnist/data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def display_image(subset):\n",
    "    nrows = 4\n",
    "    ncols = 5\n",
    "    plt.figure(figsize=(ncols*2, nrows*2))\n",
    "    \n",
    "    for i in xrange(nrows*ncols):\n",
    "        plt.subplot(nrows, ncols, i+1)\n",
    "        plt.imshow(subset[i].reshape((28,28)), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: PCA\n",
    "## (a) Extract principal components.\n",
    "After you have read in the training data, you will have to standardize the data, which means subtracting the mean and dividing by the standard deviation. Then, perform PCA to extract the principal components of the standardized training data. You may not use a library to do PCA for you, but you may use libraries (for example from numpy) to compute the singular value or eigenvalue decomposition of a matrix. Display the first 10 principal components as images. If you wish, you can then use the builtin PCA function in pyspark.mllib to compare to your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Plot variance\n",
    "Plot the variance of all of the principal componentsâ€”this corresponds to the singular\n",
    "values. This should be monotonically decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Dimension reduction\n",
    "Take a data point in the test data set and project it onto the first m principal components. Then, transform that m-length vector back into a 784-length vector and display it as an image. Repeat this for several different values of m. Also, try it on different data points. Describe the results qualitatively. Does it give an accurate representation of the images? How do the results depend on m? Can you describe what the top principal components are capturing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Plot reconstruction error\n",
    "Perform dimension reduction (as above) on the entire test data set. The reconstruction error for a data point is the sum of the squared residuals (the residual is the difference between the true pixel value and the reconstructed pixel value). Plot the reconstruction error against the number of components m used for the dimension reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: K-means\n",
    "Now use k-means to perform unsupervised clustering of the digit data, and try to assess how well the clusters capture the structure of the data. You can use the mllib implementation of k-means:\n",
    "from pyspark.mllib.clustering import KMeans The clustering is then done with the method KMeans.train:\n",
    "clusters = KMeans.train(train_data, 10, maxIterations=50,\n",
    "runs=10, initializationMode=\"random\")\n",
    "The options here limit the number of iterations of kmeans to 50, the clusters are initialized randomly, and we run the clustering 10 different times taking the one ultimately that achieves the lowest empirical risk. To evaluate how closely the clusters capture the structure of the data, associate each cluster with the majority label.\n",
    "Construct plots showing samples of the digits from each cluster, and comment on how well the clusters respect the true digit labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Classification\n",
    "## (a) Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
